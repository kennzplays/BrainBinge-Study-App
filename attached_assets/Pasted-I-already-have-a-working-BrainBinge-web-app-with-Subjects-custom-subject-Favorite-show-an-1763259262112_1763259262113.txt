I already have a working BrainBinge web app with:

- Subjects + custom subject
- Favorite show/anime input
- Question type options (free, MCQ, mixed)
- XP system
- MCQs with A/B/C/D buttons that work correctly

Right now, the FREE RESPONSE questions are too dumb: when the user types anything and clicks â€œCheck answerâ€, it basically always treats it as correct.

âš ï¸ I DO NOT want you to rewrite the whole app.  
Just modify the existing free-response grading so that correctness is actually checked using AI similarity.

ğŸ¯ Change ONLY this behavior:

1. When the user answers a free-response question and clicks â€œCheck answerâ€:
   - Send the following to the backend:
     - the original question text
     - the modelâ€™s expected answer (you already have it in the question object)
     - the userâ€™s typed answer

2. In the backend (Flask), create or update a route/function that:
   - Uses the OpenAI API (or the same model you are already using) to compare the userâ€™s answer with the model answer.
   - Ask the model to:
     - Give a similarity score from 0 to 100.
     - Decide if the answer is correct or not based on an 80% threshold.
   - Example prompt logic to the model (you can embed this in the code):

     â€œYou are grading a short free-response answer.
      Question: {question_text}
      Correct answer: {model_answer}
      Student answer: {user_answer}

      Compare the student answer with the correct answer.
      1. Give a similarity score from 0 to 100, where 100 means essentially identical in meaning.
      2. Then say whether the student answer is correct or incorrect using this rule:
         - If similarity >= 80 â†’ correct
         - If similarity < 80 â†’ incorrect

      Respond in strict JSON ONLY with:
      {
        "score": <number 0-100>,
        "is_correct": true/false,
        "feedback": "<short explanation for why it is correct or incorrect>"
      }â€

3. Parse that JSON in the backend and return it to the frontend as JSON:
   - `score`
   - `is_correct`
   - `feedback`

4. On the frontend:
   - For free-response questions, replace the old â€œeverything is correctâ€ behavior with this:
     - If `is_correct` is true:
       - Show something like: â€œâœ… Correct! [feedback]â€
       - Award the **higher XP amount** you were already giving for correct answers.
     - If `is_correct` is false:
       - Show: â€œâŒ Not quite. [feedback]â€
       - Award the **lower XP amount** for wrong answers (or no XP, depending on current logic).
   - Also show the modelâ€™s expected answer somewhere, like:
     - â€œSuggested answer: â€¦â€

5. IMPORTANT:
   - Do NOT break or change the multiple-choice question logic.
   - Do NOT remove the XP system.
   - Do NOT change the general UI or flows.
   - Only hook in this new similarity-based grading for free-response questions and wire it to the existing XP logic.

Make the minimum code changes needed in `app.py`, `script.js`, and the template so that:
- Free-response answers are judged by similarity (>=80% = correct, otherwise wrong),
- The user sees correct/incorrect with feedback,
- XP for free-response uses the same â€œmore XP if correct, less XP if wrongâ€ idea as MCQs.
